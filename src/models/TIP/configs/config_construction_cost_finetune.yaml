# TIP Fine-Tuning Config for Construction Cost Regression
# Stage 2: Supervised Fine-Tuning

defaults:
  - _self_

# Configure Hydra to use work_dir in project root instead of outputs
hydra:
  run:
    dir: ${data_base}/work_dir/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
  output_subdir: null  # Don't create outputs subdirectory

# Task
datatype: multimodal  # Required by run.py (used for checkpoint path)
eval_datatype: multimodal
strategy: tip
task: regression
use_construction_cost_dataset: true
algorithm_name: TIP  # Required by run.py
target: construction_cost  # Required by run.py (for logging and exp name)

# Base directory for data paths (project root)
data_base: /hdd/hiep/CODE/Construction_Cost_Prediction

# Checkpoint from pretraining
checkpoint: null  # Set to: work_dir/runs/multimodal/tip_pretrain_*/checkpoint_best_*.ckpt

# Data paths (absolute paths)
# Note: For fine-tuning, we need ground truth, so we use train/val splits (NOT test set)

# K-Fold Cross-Validation Options
use_kfold: True  # If true, use k-fold CV from unified trainval.csv; if false, use fixed train/val splits
k_fold: 5  # Number of folds (only used if use_kfold=true)
k_fold_seed: 42  # Random seed for k-fold splitting (only used if use_kfold=true)
k_fold_current: -1  # Current fold to use (0-indexed, 0 to k_fold-1). Set to -1 (default) to run all folds automatically (only used if use_kfold=true)
data_trainval_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean.csv  # Unified trainval CSV (preprocessed, only used if use_kfold=true)

# Fixed Split Options (only used if use_kfold=false)
data_train_eval_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/train/train_clean.csv
data_val_eval_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/val/val_clean.csv  # Val split (has ground truth)
train_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/train/train_clean_metadata.pkl
val_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/val/val_clean_metadata.pkl

# Common paths (used for both k-fold and fixed split)
composite_dir_trainval: /hdd/hiep/CODE/Construction_Cost_Prediction/data/trainval_composite  # For train/val sets (both use this)
composite_dir_test: /hdd/hiep/CODE/Construction_Cost_Prediction/data/test_composite  # For test set (only for final inference, no training)
field_lengths_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/field_lengths.pt
trainval_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean_metadata.pkl  # Metadata for unified trainval (used if use_kfold=true)
labels_train_eval_imaging: null  # Not needed for fine-tuning - targets are in CSV file
labels_val_eval_imaging: null  # Not needed for fine-tuning - targets are in CSV file

# Model (inherited from checkpoint)
use_satellite_encoder: true
use_satlas: true
sentinel2_model_id: Sentinel2_SwinB_SI_MS
satellite_feature_dim: 512

# Fine-Tuning Strategy
finetune_strategy: frozen  # Freeze entire pretrained backbone (image + tabular + multimodal encoders)
freeze_image: true  # Freeze image encoder (redundant if finetune_strategy=frozen, but kept for clarity)
freeze_tabular: true  # Freeze tabular encoder (redundant if finetune_strategy=frozen, but kept for clarity)
missing_tabular: false
# Note: Only the regression head (classifier) will be trained when finetune_strategy=frozen

# Regression Head
num_classes: 1
# regression_loss must be a dict with loss names and weights
# Only losses in this dict will contribute to the total weighted loss for backprop
# All losses (rmsle, mae, rmse, mse, huber) are calculated internally for monitoring
regression_loss:
  rmsle: 0.5  # Primary metric (competition)
  mae: 0.2    # Interpretable error
  mse: 0.2    # Penalize large errors
  rmse: 0.1   # Root mean squared error
huber_delta: 1.0  # Only used if huber is in regression_loss
target_log_transform: true
target_mean: 6.513477  # From preprocessing (log space) - must match pretrain config
target_std: 1.101045   # From preprocessing (log space) - must match pretrain config

# Training
batch_size: 16
lr: 3e-4  # Pretrain LR (not used in fine-tuning, but required by run.py)
weight_decay: 1e-5  # Pretrain weight decay (not used in fine-tuning, but required by run.py)
lr_eval: 1e-4  # Fine-tuning learning rate (used for regression head)
weight_decay_eval: 1e-5  # Fine-tuning weight decay
max_epochs: 50
log_every_n_steps: 1  # Log every step (since we have small number of batches ~13)

# TIP Loss Parameters (not used in fine-tuning, but required by run.py)
corruption_rate: 0.0  # Not used during fine-tuning
temperature: 0.1  # Not used during fine-tuning
replace_special_rate: 0.0  # Not used during fine-tuning
check_val_every_n_epoch: 1
val_check_interval: 1.0  # Check validation every epoch
eval_metric: rmsle  # Metric to monitor for best checkpoint (rmsle, mae, rmse) - rmsle is primary
regression_head_class: RegressionMLP  # Head class name from ConstructionCostHead module (must match class name exactly)
regression_head_dropout: 0.2  # Dropout probability for regression head
limit_train_batches: null  # Use all training batches
limit_val_batches: null  # Use all validation batches
limit_test_batches: null  # Use all test batches
gpus: 1
num_workers: 4
pin_memory: true
seed: 42
resume_training: false
test_and_eval: false  # Set to true to run test set evaluation after training

# Image
img_size: 224
eval_train_augment_rate: 0.0  # No augmentation for fine-tuning
live_loading: false
augmentation_speedup: true
eval_one_hot: false
delete_segmentation: false

# Satellite-specific
use_sentinel2: true
use_viirs: true

# Training Mode Flags
pretrain: false  # Set to true for pretraining
finetune: true   # Set to true for fine-tuning (construction cost specific)
evaluate: false  # Set to true for generic evaluation/fine-tuning (other tasks)
test: false     # Set to true for testing
generate_embeddings: false  # Set to true to generate embeddings only

# Logging
wandb_project: construction_cost_tip
wandb_name: tip_finetune
use_wandb: true  # Enable WandB online logging
offline: false  # Set to false for online WandB logging
wandb_entity: null  # Set to your WandB entity/username if needed
wandb_id: null  # For resuming training
exp_name: tip_finetune
comment: "TIP fine-tuning for construction cost regression (frozen backbone)"

