# TIP Pretraining Config for Construction Cost Prediction
# Stage 1: Self-Supervised Pretraining

defaults:
  - _self_

# Configure Hydra to use work_dir in project root instead of outputs
hydra:
  run:
    dir: ${data_base}/work_dir/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
  output_subdir: null  # Don't create outputs subdirectory

# Dataset
datatype: multimodal
strategy: tip
use_construction_cost_dataset: true

# Base directory for data paths (project root)
data_base: /hdd/hiep/CODE/Construction_Cost_Prediction

# Data paths (absolute paths)

# K-Fold Cross-Validation Options
use_kfold: False  # If true, use k-fold CV from unified trainval.csv; if false, use fixed train/val splits
k_fold: 5  # Number of folds (only used if use_kfold=true)
k_fold_seed: 42  # Random seed for k-fold splitting (only used if use_kfold=true)
k_fold_current: -1  # Current fold to use (0-indexed, 0 to k_fold-1). Set to -1 (default) to run all folds automatically (only used if use_kfold=true)
data_trainval_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean.csv  # Unified trainval CSV (preprocessed, only used if use_kfold=true)

# Fixed Split Options (only used if use_kfold=false)
# data_train_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/train/train_clean.csv

# for pretrain, we use full set to get all features over trainval
data_train_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean.csv
data_val_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/val/val_clean.csv
# train_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/train/train_clean_metadata.pkl
train_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean_metadata.pkl
val_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/val/val_clean_metadata.pkl

# Common paths (used for both k-fold and fixed split)
composite_dir_trainval: /hdd/hiep/CODE/Construction_Cost_Prediction/data/trainval_composite  # For train and val sets
composite_dir_test: /hdd/hiep/CODE/Construction_Cost_Prediction/data/test_composite  # For test set
field_lengths_tabular: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/field_lengths.pt
trainval_metadata_path: /hdd/hiep/CODE/Construction_Cost_Prediction/data/annotation/trainval/trainval_clean_metadata.pkl  # Metadata for unified trainval (used if use_kfold=true)
labels_train: null  # Will be auto-generated
labels_val: null    # Will be auto-generated

# Pretrained checkpoints (optional)
imaging_pretrain_checkpoint: null  # Path to pretrained imaging encoder checkpoint
pretrained_imaging_strategy: trainable  # Options: frozen, trainable
tabular_pretrain_checkpoint: null  # Path to pretrained tabular encoder checkpoint
pretrained_tabular_strategy: frozen  # Options: frozen, trainable

# Model Architecture
model: satlas  # Options: satlas, resnet18, resnet50
use_satellite_encoder: true
use_satlas: true
sentinel2_model_id: Sentinel2_SwinB_SI_MS
satellite_feature_dim: 512
freeze_backbone: false
use_fpn: true

# Tabular Encoder
tabular_embedding_dim: 512
tabular_transformer_num_layers: 4
num_cat: 16  # 16 categorical features (from preprocessing)
num_con: 3   # 3 numerical features (from preprocessing)
embedding_dropout: 0.1
drop_rate: 0.1

# Multimodal Encoder
multimodal_embedding_dim: 512
multimodal_transformer_num_layers: 4

# Projection Heads
embedding_dim: 2048
projection_dim: 128

# Training
batch_size: 32
lr: 3e-4
weight_decay: 1e-5
lr_finetune: 1e-3  # For fine-tuning stage
weight_decay_finetune: 1e-5  # For fine-tuning stage
max_epochs: 100
check_val_every_n_epoch: 5
log_every_n_steps: 1  # Log every step (since we have small number of batches)
gpus: 1
num_workers: 4
pin_memory: true

# Scheduler
scheduler: anneal  # Options: cosine, anneal
warmup_epochs: 10  # Warmup epochs for anneal scheduler
anneal_max_epochs: ${max_epochs}  # Max epochs for anneal scheduler (usually same as max_epochs)
cosine_anneal_mult: 1.0  # Multiplier for cosine scheduler (if using cosine)
dataset_length: null  # Will be set automatically from dataset size

# Online evaluation (optional)
classifier_freq: 10  # Frequency (in epochs) to run online classifier evaluation
log_images: false  # Whether to log images during training

# TIP Loss Parameters
corruption_rate: 0.3
replace_random_rate: 0.0
replace_special_rate: 0.5
temperature: 0.1
lambda_0: 0.5  # Weight for ITC loss
loss: clip  # Loss type for contrastive learning
view: augmented  # View type for contrastive learning
tr_loss: normal  # Tabular reconstruction loss type

# Image Augmentation
img_size: 224  # Increased to match SatlasPretrain training size (was 224)
augmentation_rate: 0.95
live_loading: false
augmentation_speedup: true
one_hot: false
delete_segmentation: false

# Satellite-specific
use_sentinel2: true
use_viirs: true

# Logging
wandb_project: construction_cost_tip  # WandB project name (groups all experiments)
wandb_name: tip_pretrain  # Legacy parameter (not used in current code)
use_wandb: true  # Enable WandB online logging
offline: false  # Set to false for online WandB logging
wandb_entity: null  # Set to your WandB entity/username if needed
wandb_id: null  # For resuming training
exp_name: tip_pretrain_${regression_head.type}  # WandB experiment name (will have timestamp added in code): tip_pretrain_QueryAttentionRegression_1221_0245
target: construction_cost  # For logging/display purposes only
comment: "TIP pretraining for construction cost prediction"
seed: 42
pretrain: true
finetune: false
evaluate: false
test: false
resume_training: false
checkpoint: null
generate_embeddings: false  # Set to true to generate embeddings only
algorithm_name: TIP
online_mlp: true  # Enable online regression evaluation
num_classes: 1  # For regression
# # Regression Head Configuration (all head-related parameters in one dict)
# regression_head:
#   type: RegressionMLP  # Head class name: RegressionMLP or MixtureOfExpertsRegression
#   p: 0.2  # Dropout probability
#   n_hidden: 2048  # Hidden dimension (null = use n_input, which is embedding_dim)
#   # Loss configuration (dict with loss names and weights)
#   # Only losses in this dict will contribute to the total weighted loss for backprop
#   # All losses (rmsle, mae, rmse, mse, huber) are calculated internally for monitoring
#   loss_type:
#     rmsle: 1.0  # Primary metric (competition)
#     # mae: 0.2    # Interpretable error
#     # mse: 0.2    # Penalize large errors
#     # rmse: 0.1   # Root mean squared error
#     # huber: 0.5  # Huber loss
#   target_mean: 6.513477  # Mean of log(1 + target) values (computed from training dataset)
#   target_std: 1.101045   # Std of log(1 + target) values (computed from training dataset)
#   target_log_transform: true  # Whether targets are log-transformed (log1p)
#   huber_delta: 1.0  # Delta parameter for Huber loss
  # For MixtureOfExpertsRegression, add:
  # num_experts: 8
  # top_k: 2
  # use_load_balancing: false
  # load_balancing_weight: 0.01


# regression_head:
#   type: MixtureOfExpertsRegression  # Head class: RegressionMLP or MixtureOfExpertsRegression
#   p: 0.2  # Dropout probability
#   n_hidden: 2048  # Hidden dimension (null = use n_input, which is embedding_dim)
#   # Loss configuration (dict with loss names and weights)
#   # Only losses in this dict will contribute to the total weighted loss for backprop
#   # All losses (rmsle, mae, rmse, mse, huber) are calculated internally for monitoring
#   loss_type:
#     rmsle: 1.0  # Primary metric (competition)
#     # mae: 0.2    # Interpretable error
#     # mse: 0.2    # Penalize large errors
#     # rmse: 0.1   # Root mean squared error
#   target_mean: 6.513477  # From preprocessing (log space) - must match pretrain config
#   target_std: 1.101045   # From preprocessing (log space) - must match pretrain config
#   target_log_transform: true
#   huber_delta: 1.0  # Delta parameter for Huber loss
#   # MoE-specific parameters:
#   num_experts: 4  # Number of expert networks (reduced from 8 for better convergence with small dataset)
#   top_k: 2  # Top-K routing: number of experts to use per sample
#   use_load_balancing: false  # Enable load balancing loss
#   load_balancing_weight: 0.01  # Weight for load balancing loss

# regression_head:
#   type: AttentionAggregationRegression  # Head class: RegressionMLP or AttentionAggregationRegression
#   p: 0.2  # Dropout probability
#   n_hidden: 2048  # Hidden dimension (null = use n_input, which is embedding_dim)
#   num_heads: 8  # Number of attention heads (for AttentionAggregationRegression)
#   # Loss configuration (dict with loss names and weights)
#   # Only losses in this dict will contribute to the total weighted loss for backprop
#   # All losses (rmsle, mae, rmse, mse, huber) are calculated internally for monitoring
#   loss_type:
#     rmsle: 1.0  # Primary metric (competition)
#     # mae: 0.2    # Interpretable error
#     # mse: 0.2    # Penalize large errors
#     # rmse: 0.1   # Root mean squared error
#   target_mean: 6.513477  # From preprocessing (log space) - must match pretrain config
#   target_std: 1.101045   # From preprocessing (log space) - must match pretrain config
#   target_log_transform: true
#   huber_delta: 1.0  # Delta parameter for Huber loss


# Example alternatives (commented out — enable one at a time when testing new heads)
regression_head:
  type: QueryAttentionRegression
  p: 0.2
  n_hidden: 2048
  num_queries: 1      # learnable queries (1–4 typical)
  num_heads: 8        # attention heads for query pooling
  loss_type:
    rmsle: 1.0
  target_mean: 6.513477
  target_std: 1.101045
  target_log_transform: true
  huber_delta: 1.0
#
# regression_head:
#   type: GatedAttentionPoolingRegression
#   p: 0.2
#   n_hidden: 2048
#   gate_hidden: 256    # optional, defaults to n_input//2
#   loss_type:
#     rmsle: 1.0
#   target_mean: 6.513477
#   target_std: 1.101045
#   target_log_transform: true
#   huber_delta: 1.0
#
# regression_head:
#   type: DualPoolRegression
#   p: 0.2
#   n_hidden: 2048
#   loss_type:
#     rmsle: 1.0
#   target_mean: 6.513477
#   target_std: 1.101045
#   target_log_transform: true
#   huber_delta: 1.0

eval_metric: rmsle  # Metric to monitor for best checkpoint (rmsle, mae, rmse) - rmsle is primary
limit_train_batches: null
limit_val_batches: null
enable_progress_bar: true

